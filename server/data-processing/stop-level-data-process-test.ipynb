{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import dirname\n",
    "import warnings\n",
    "import geopandas as gpd\n",
    "\n",
    "## from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from joblib import dump\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced lag IDs\n",
      "Converted NAs to np.nan\n"
     ]
    }
   ],
   "source": [
    "is_for_deploy = False\n",
    "\n",
    "runtime = pd.read_parquet(\"C:/Users/huyh/Documents/Penn/Spring 2023/Cloud Computing/cloud-computing-bus-bunching/server/raw-data/runtimeDf.gzip\")\n",
    "\n",
    "runtime = runtime.query(\"routeId.isin(['21', '33', '47'])\").copy()\n",
    "\n",
    "toJoinFrom = runtime.copy().dropna(subset=[\"instanceId\"])\n",
    "same_trip_cols = [\"serviceDate\", \"routeId\", \"directionId\", \"tripId\"]\n",
    "\n",
    "toJoin = toJoinFrom[[\"instanceId\", \"prevInstanceId\"]].dropna(\n",
    "    subset=[\"instanceId\", \"prevInstanceId\"]\n",
    ")\n",
    "\n",
    "runtimeDf = runtime.copy()\n",
    "\n",
    "for lagSteps in range(1, 22):\n",
    "    # First get lag trips\n",
    "    runtimeDf = runtimeDf.sort_values(same_trip_cols + [\"toStopSequence\"])\n",
    "    runtimeDf[f\"lag{lagSteps}InstanceId\"] = runtimeDf.groupby(same_trip_cols)[\n",
    "        \"instanceId\"\n",
    "    ].shift(lagSteps)\n",
    "\n",
    "    # Then get prev buses of lag trips\n",
    "    thisToJoin = toJoin.copy().rename(\n",
    "        columns={\n",
    "            \"instanceId\": f\"lag{lagSteps}InstanceId\",\n",
    "            \"prevInstanceId\": f\"lag{lagSteps}PrevInstanceId\",\n",
    "        }\n",
    "    )\n",
    "    runtimeDf = runtimeDf.merge(thisToJoin, how=\"left\", on=f\"lag{lagSteps}InstanceId\")\n",
    "\n",
    "print(\"Produced lag IDs\")\n",
    "\n",
    "runtimeDf = runtimeDf.applymap(lambda x: np.nan if x is pd.NA else x)\n",
    "\n",
    "print(\"Converted NAs to np.nan\")\n",
    "\n",
    "lag_vars = [\"headway\", \"speed\", \"late\"]\n",
    "\n",
    "for lagSteps in range(1, 22):\n",
    "    # Join to lag\n",
    "\n",
    "    thisToJoin = (\n",
    "        toJoinFrom.copy()[[\"instanceId\"] + lag_vars]\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"instanceId\": f\"lag{lagSteps}InstanceId\",\n",
    "                \"headway\": f\"headwayLag{lagSteps}\",\n",
    "                \"speed\": f\"speedLag{lagSteps}\",\n",
    "                \"late\": f\"lateLag{lagSteps}\",\n",
    "            }\n",
    "        )\n",
    "        .dropna(subset=[f\"lag{lagSteps}InstanceId\"])\n",
    "    )\n",
    "\n",
    "    runtimeDf = runtimeDf.merge(thisToJoin, how=\"left\", on=f\"lag{lagSteps}InstanceId\")\n",
    "\n",
    "    # Join to prev of lag\n",
    "\n",
    "    thisToJoin = (\n",
    "        toJoinFrom.copy()[[\"instanceId\"] + lag_vars]\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"instanceId\": f\"lag{lagSteps}PrevInstanceId\",\n",
    "                \"headway\": f\"prevBus_headwayLag{lagSteps}\",\n",
    "                \"speed\": f\"prevBus_speedLag{lagSteps}\",\n",
    "                \"late\": f\"prevBus_lateLag{lagSteps}\",\n",
    "            }\n",
    "        )\n",
    "        .dropna(subset=[f\"lag{lagSteps}PrevInstanceId\"])\n",
    "    )\n",
    "\n",
    "    runtimeDf = runtimeDf.merge(\n",
    "        thisToJoin, how=\"left\", on=f\"lag{lagSteps}PrevInstanceId\"\n",
    "    )\n",
    "\n",
    "for lagSteps in range(1, 21):\n",
    "    for var in lag_vars:\n",
    "        runtimeDf[f\"{var}Lag{lagSteps}Diff{lagSteps+1}\"] = (\n",
    "            runtimeDf[f\"{var}Lag{lagSteps}\"] - runtimeDf[f\"{var}Lag{lagSteps+1}\"]\n",
    "        )\n",
    "        runtimeDf[f\"prevBus_{var}Lag{lagSteps}Diff{lagSteps+1}\"] = (\n",
    "            runtimeDf[f\"prevBus_{var}Lag{lagSteps}\"]\n",
    "            - runtimeDf[f\"prevBus_{var}Lag{lagSteps+1}\"]\n",
    "        )\n",
    "\n",
    "# stops = gpd.read_file(f\"{server_dir}/raw-data/stops/stopsGeographyProcessed.shp\")\n",
    "# stops = stops.rename(columns={\"directionI\": \"directionId\", \"StopId\": \"stopId\"}).drop(\n",
    "#     \"geography\", axis=1\n",
    "# )\n",
    "\n",
    "# stops.routeId = stops.routeId.astype(str)\n",
    "# stops.directionId = stops.directionId.astype(str)\n",
    "# stops.stopId = stops.stopId.astype(str)\n",
    "\n",
    "# stops = stops.drop_duplicates(subset=[\"routeId\", \"directionId\", \"stopId\"])\n",
    "\n",
    "# runtimeDf = runtimeDf.merge(\n",
    "#     stops[[\"routeId\", \"directionId\", \"stopId\", \"centerCity\"]],\n",
    "#     how=\"left\",\n",
    "#     left_on=[\"routeId\", \"directionId\", \"toStopId\"],\n",
    "#     right_on=[\"routeId\", \"directionId\", \"stopId\"],\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_level = pd.read_csv(\"C:/Users/huyh/Documents/Penn/Spring 2023/Cloud Computing/cloud-computing-bus-bunching/server/raw-data/stops_spatial_lag.csv\", \n",
    "                         index_col=False)\n",
    "stop_level = stop_level.drop('toStopSequence',  axis = 1)\n",
    "stop_level.routeId = stop_level.routeId.astype(str)\n",
    "stop_level.directionId = stop_level.directionId.astype(str).apply(lambda x: x.split('.')[0])\n",
    "stop_level.toStopId = stop_level.toStopId.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDf = runtimeDf.merge(stop_level, how = \"left\", on = ['routeId', 'directionId', 'toStopId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoW                     object\n",
      "serviceDate     datetime64[ns]\n",
      "routeId                 object\n",
      "directionId             object\n",
      "blockId                  int64\n",
      "                     ...      \n",
      "sumComm_15             float64\n",
      "sumComm_20             float64\n",
      "pctSignal_15           float64\n",
      "pctSignal_10           float64\n",
      "pctSignal_20           float64\n",
      "Length: 360, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(runtimeDf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_col = ['sumRiders_10', 'sumRiders_20', 'sumComm_10', 'sumComm_20', 'pctSignal_10', 'pctSignal_20', 'pop','popDen', 'riders', 'commuter', 'comm_count' ]\n",
    "mean = runtimeDf[fill_na_col].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDf[fill_na_col] = runtimeDf[fill_na_col].fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDf_variables = runtimeDf[['sumRiders_10', 'sumRiders_20', 'sumComm_10', 'sumComm_20', 'pctSignal_10', 'pctSignal_20', 'pop','popDen', 'riders', 'commuter', 'comm_count', 'routeId', 'toStopId', 'directionId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDf_variables.to_parquet(\"C:/Users/huyh/Documents/Penn/Spring 2023/Cloud Computing/cloud-computing-bus-bunching/server/raw-data/stop_level_var.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = pd.read_parquet(\"C:/Users/huyh/Documents/Penn/Spring 2023/Cloud Computing/cloud-computing-bus-bunching/server/raw-data/runtimeDf.gzip\")\n",
    "routes = [\"21\", \"33\", \"47\"]\n",
    "stop_info = (\n",
    "    runtime.query(\"routeId.isin(@routes)\")\n",
    "    .copy()\n",
    "    .groupby([\"routeId\", \"directionId\", \"toStopId\"])\n",
    "    .agg({ \"toStopSequence\": \"min\",\n",
    "         \"stopPathLength\": \"mean\",\n",
    "            \"expectedCumRuntimeSeconds\": \"mean\", \n",
    "            \"DoW\": \"size\"})\n",
    "    .query(\"DoW > 0\")\n",
    "    .copy()\n",
    "    .reset_index()\n",
    "    .drop([\"DoW\"], axis=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_parquet(\"C:/Users/huyh/Documents/Penn/Spring 2023/Cloud Computing/cloud-computing-bus-bunching/server/raw-data/stop_level_var.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_info = stop_info.merge(\n",
    "    z, how=\"left\", on=[\"routeId\", \"directionId\", \"toStopId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_info = stop_info.drop_duplicates(subset=[\"routeId\", \"directionId\", \"toStopId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['toStopSequence', 'sumRiders_10', 'sumRiders_20', 'sumComm_10', 'sumComm_20', 'pctSignal_10', 'pctSignal_20', 'pop','popDen', 'riders', 'commuter', 'comm_count' ]\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_info[cols_to_scale] = scaler.fit_transform(stop_info[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_info[\"stop_unique_id\"] = (\n",
    "    stop_info[\"routeId\"] + \"_\" + stop_info[\"directionId\"] + \"_\" + stop_info[\"toStopId\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next 1-20 stops by stop\n",
    "sorted = stop_info.sort_values(\n",
    "    [\n",
    "        \"routeId\",\n",
    "        \"directionId\",\n",
    "        \"toStopSequence\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in range(1, 22):\n",
    "    sorted[f\"next_{i}_unique_id\"] = sorted.groupby([\"routeId\", \"directionId\"])[\n",
    "        \"stop_unique_id\"\n",
    "    ].shift(-i)\n",
    "\n",
    "next_stops = sorted.drop(\n",
    "    [    \"stopPathLength\",\n",
    "        \"expectedCumRuntimeSeconds\",\n",
    "        \"toStopSequence\",\n",
    "        \"routeId\",\n",
    "        \"directionId\",\n",
    "        \"toStopId\",\n",
    "        'sumRiders_10', \n",
    "        'sumRiders_20', \n",
    "        'sumComm_10',\n",
    "        'sumComm_20', \n",
    "        'pctSignal_10', \n",
    "        'pctSignal_20',\n",
    "        'pop',\n",
    "        'popDen', \n",
    "        'riders', \n",
    "        'commuter', \n",
    "        'comm_count'\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "stop_info = stop_info.drop([\"routeId\", \"directionId\", \"toStopId\"], axis=1)\n",
    "stop_info = stop_info.convert_dtypes()\n",
    "\n",
    "stop_info_dict = stop_info.set_index(\"stop_unique_id\")[\n",
    "    [   \"stopPathLength\",\n",
    "        \"expectedCumRuntimeSeconds\",\n",
    "        'toStopSequence',\n",
    "        'sumRiders_10', \n",
    "        'sumRiders_20', \n",
    "        'sumComm_10', \n",
    "        'sumComm_20', \n",
    "        'pctSignal_10', \n",
    "        'pctSignal_20', \n",
    "        'pop',\n",
    "        'popDen', \n",
    "        'riders', \n",
    "        'commuter', \n",
    "        'comm_count' \n",
    "    ]\n",
    "].to_dict(orient=\"index\")\n",
    "\n",
    "next_stops = next_stops.convert_dtypes()\n",
    "next_stops_dict = next_stops.set_index(\"stop_unique_id\").to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = pd.read_csv(\"../raw-data/trip_schedule.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77b223090ca2c5cd3aff98c500c849405635d50fc52e0f61cf1e5754cc461253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
